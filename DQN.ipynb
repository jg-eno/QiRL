{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qiskit.circuit import QuantumCircuit, QuantumRegister, Parameter, ParameterVector, ParameterExpression\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "\n",
    "import qiskit as qk\n",
    "from qiskit_aer.primitives import EstimatorV2 as AerEstimator\n",
    "\n",
    "import qiskit_machine_learning as qml\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch \n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import LBFGS, SGD, Adam, RMSprop\n",
    "\n",
    "import gymnasium as gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25306551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better anumation in Jupyter NBK\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72124c8d",
   "metadata": {},
   "source": [
    "### Rx(θ) Gate\n",
    "\n",
    "The **rotation around the X-axis** (Rx) gate is defined as:\n",
    "\n",
    "$$\n",
    "R_x(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos\\left(\\frac{\\theta}{2}\\right) & -i\\sin\\left(\\frac{\\theta}{2}\\right) \\\\\n",
    "-i\\sin\\left(\\frac{\\theta}{2}\\right) & \\cos\\left(\\frac{\\theta}{2}\\right)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "It performs a rotation of the qubit state vector by an angle **θ** about the **X-axis** of the Bloch sphere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d1008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Circuit (EWncoding classical information onto a quantum register)\n",
    "def encoding_circuit(inputs, num_qubits = 4, *args):\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    for i in range(len(inputs)):\n",
    "        qc.rx(inputs[i],i)\n",
    "\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b0b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameterized Quantum Circuit : An alternative to DQN, to estimate the Q-value (reward), based on the (State, Action) pair\n",
    "def parametrized_circuit(num_qubits = 4, reuploading = False, reps = 2, insert_barriers = True, meas = False):\n",
    "    qr = qk.QuantumRegister(num_qubits, 'qr')\n",
    "    qc = qk.QuantumCircuit(qr)\n",
    "    \n",
    "    if meas:\n",
    "        qr = qk.QuantumRegister(num_qubits, 'qr')\n",
    "        cr = qk.ClassicalRegister(num_qubits, 'cr')\n",
    "        qc = qk.QuantumCircuit(qr,cr)\n",
    "    \n",
    "    \n",
    "    if not reuploading:\n",
    "        \n",
    "        # Define a vector containg Inputs as parameters (*not* to be optimized)\n",
    "        inputs = qk.circuit.ParameterVector('x', num_qubits)\n",
    "                \n",
    "        # Encode classical input data\n",
    "        qc.compose(encoding_circuit(inputs, num_qubits = num_qubits), inplace = True)\n",
    "        if insert_barriers: qc.barrier()\n",
    "        \n",
    "        # Variational circuit\n",
    "        qc.compose(TwoLocal(num_qubits, ['ry', 'rz'], 'cz', 'circular', \n",
    "               reps=reps, insert_barriers= insert_barriers, \n",
    "               skip_final_rotation_layer = True), inplace = True)\n",
    "        if insert_barriers: qc.barrier()\n",
    "        \n",
    "        # Add final measurements\n",
    "        if meas: qc.measure(qr,cr)\n",
    "        \n",
    "    elif reuploading:\n",
    "        \n",
    "        # Define a vector containg Inputs as parameters (*not* to be optimized)\n",
    "        inputs = qk.circuit.ParameterVector('x', num_qubits)\n",
    "                \n",
    "        # Define a vector containng variational parameters\n",
    "        θ = qk.circuit.ParameterVector('θ', 2 * num_qubits * reps)\n",
    "        \n",
    "        # Iterate for a number of repetitions\n",
    "        for rep in range(reps):\n",
    "\n",
    "            # Encode classical input data\n",
    "            qc.compose(encoding_circuit(inputs, num_qubits = num_qubits), inplace = True)\n",
    "            if insert_barriers: qc.barrier()\n",
    "                \n",
    "            # Variational circuit (does the same as TwoLocal from Qiskit)\n",
    "            for qubit in range(num_qubits):\n",
    "                qc.ry(θ[qubit + 2*num_qubits*(rep)], qubit)\n",
    "                qc.rz(θ[qubit + 2*num_qubits*(rep) + num_qubits], qubit)\n",
    "            if insert_barriers: qc.barrier()\n",
    "                \n",
    "            # Add entanglers (this code is for a circular entangler)\n",
    "            qc.cz(qr[-1], qr[0])\n",
    "            for qubit in range(num_qubits-1):\n",
    "                qc.cz(qr[qubit], qr[qubit+1])\n",
    "            if insert_barriers: qc.barrier()\n",
    "                        \n",
    "        # Add final measurements\n",
    "        if meas: qc.measure(qr,cr)\n",
    "        \n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161833cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">      ┌──────────┐ ░ ┌──────────┐┌──────────┐ ░              ░ ┌──────────┐ ░ »\n",
       "qr_0: ┤ Rx(x[0]) ├─░─┤ Ry(θ[0]) ├┤ Rz(θ[4]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │  │        ░ ├──────────┤ ░ »\n",
       "qr_1: ┤ Rx(x[1]) ├─░─┤ Ry(θ[1]) ├┤ Rz(θ[5]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │     │     ░ ├──────────┤ ░ »\n",
       "qr_2: ┤ Rx(x[2]) ├─░─┤ Ry(θ[2]) ├┤ Rz(θ[6]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │        │  ░ ├──────────┤ ░ »\n",
       "qr_3: ┤ Rx(x[3]) ├─░─┤ Ry(θ[3]) ├┤ Rz(θ[7]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─»\n",
       "      └──────────┘ ░ └──────────┘└──────────┘ ░              ░ └──────────┘ ░ »\n",
       "«       ┌──────────┐┌───────────┐ ░              ░ ┌──────────┐ ░ ┌───────────┐»\n",
       "«qr_0: ─┤ Ry(θ[8]) ├┤ Rz(θ[12]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[16]) ├»\n",
       "«       ├──────────┤├───────────┤ ░  │  │        ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_1: ─┤ Ry(θ[9]) ├┤ Rz(θ[13]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[17]) ├»\n",
       "«      ┌┴──────────┤├───────────┤ ░  │     │     ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_2: ┤ Ry(θ[10]) ├┤ Rz(θ[14]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[18]) ├»\n",
       "«      ├───────────┤├───────────┤ ░  │        │  ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_3: ┤ Ry(θ[11]) ├┤ Rz(θ[15]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[19]) ├»\n",
       "«      └───────────┘└───────────┘ ░              ░ └──────────┘ ░ └───────────┘»\n",
       "«      ┌───────────┐ ░              ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐»\n",
       "«qr_0: ┤ Rz(θ[20]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[24]) ├┤ Rz(θ[28]) ├»\n",
       "«      ├───────────┤ ░  │  │        ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_1: ┤ Rz(θ[21]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[25]) ├┤ Rz(θ[29]) ├»\n",
       "«      ├───────────┤ ░  │     │     ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_2: ┤ Rz(θ[22]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[26]) ├┤ Rz(θ[30]) ├»\n",
       "«      ├───────────┤ ░  │        │  ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_3: ┤ Rz(θ[23]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[27]) ├┤ Rz(θ[31]) ├»\n",
       "«      └───────────┘ ░              ░ └──────────┘ ░ └───────────┘└───────────┘»\n",
       "«       ░              ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐ ░          »\n",
       "«qr_0: ─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[32]) ├┤ Rz(θ[36]) ├─░──■──■────»\n",
       "«       ░  │  │        ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │  │    »\n",
       "«qr_1: ─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[33]) ├┤ Rz(θ[37]) ├─░──┼──■──■─»\n",
       "«       ░  │     │     ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │     │ »\n",
       "«qr_2: ─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[34]) ├┤ Rz(θ[38]) ├─░──┼─────■─»\n",
       "«       ░  │        │  ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │       »\n",
       "«qr_3: ─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[35]) ├┤ Rz(θ[39]) ├─░──■───────»\n",
       "«       ░              ░ └──────────┘ ░ └───────────┘└───────────┘ ░          »\n",
       "«          ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐ ░              ░ \n",
       "«qr_0: ────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[40]) ├┤ Rz(θ[44]) ├─░──■──■────────░─\n",
       "«          ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │  │        ░ \n",
       "«qr_1: ────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[41]) ├┤ Rz(θ[45]) ├─░──┼──■──■─────░─\n",
       "«          ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │     │     ░ \n",
       "«qr_2: ─■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[42]) ├┤ Rz(θ[46]) ├─░──┼─────■──■──░─\n",
       "«       │  ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │        │  ░ \n",
       "«qr_3: ─■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[43]) ├┤ Rz(θ[47]) ├─░──■────────■──░─\n",
       "«          ░ └──────────┘ ░ └───────────┘└───────────┘ ░              ░ </pre>"
      ],
      "text/plain": [
       "      ┌──────────┐ ░ ┌──────────┐┌──────────┐ ░              ░ ┌──────────┐ ░ »\n",
       "qr_0: ┤ Rx(x[0]) ├─░─┤ Ry(θ[0]) ├┤ Rz(θ[4]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │  │        ░ ├──────────┤ ░ »\n",
       "qr_1: ┤ Rx(x[1]) ├─░─┤ Ry(θ[1]) ├┤ Rz(θ[5]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │     │     ░ ├──────────┤ ░ »\n",
       "qr_2: ┤ Rx(x[2]) ├─░─┤ Ry(θ[2]) ├┤ Rz(θ[6]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─»\n",
       "      ├──────────┤ ░ ├──────────┤├──────────┤ ░  │        │  ░ ├──────────┤ ░ »\n",
       "qr_3: ┤ Rx(x[3]) ├─░─┤ Ry(θ[3]) ├┤ Rz(θ[7]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─»\n",
       "      └──────────┘ ░ └──────────┘└──────────┘ ░              ░ └──────────┘ ░ »\n",
       "«       ┌──────────┐┌───────────┐ ░              ░ ┌──────────┐ ░ ┌───────────┐»\n",
       "«qr_0: ─┤ Ry(θ[8]) ├┤ Rz(θ[12]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[16]) ├»\n",
       "«       ├──────────┤├───────────┤ ░  │  │        ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_1: ─┤ Ry(θ[9]) ├┤ Rz(θ[13]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[17]) ├»\n",
       "«      ┌┴──────────┤├───────────┤ ░  │     │     ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_2: ┤ Ry(θ[10]) ├┤ Rz(θ[14]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[18]) ├»\n",
       "«      ├───────────┤├───────────┤ ░  │        │  ░ ├──────────┤ ░ ├───────────┤»\n",
       "«qr_3: ┤ Ry(θ[11]) ├┤ Rz(θ[15]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[19]) ├»\n",
       "«      └───────────┘└───────────┘ ░              ░ └──────────┘ ░ └───────────┘»\n",
       "«      ┌───────────┐ ░              ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐»\n",
       "«qr_0: ┤ Rz(θ[20]) ├─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[24]) ├┤ Rz(θ[28]) ├»\n",
       "«      ├───────────┤ ░  │  │        ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_1: ┤ Rz(θ[21]) ├─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[25]) ├┤ Rz(θ[29]) ├»\n",
       "«      ├───────────┤ ░  │     │     ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_2: ┤ Rz(θ[22]) ├─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[26]) ├┤ Rz(θ[30]) ├»\n",
       "«      ├───────────┤ ░  │        │  ░ ├──────────┤ ░ ├───────────┤├───────────┤»\n",
       "«qr_3: ┤ Rz(θ[23]) ├─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[27]) ├┤ Rz(θ[31]) ├»\n",
       "«      └───────────┘ ░              ░ └──────────┘ ░ └───────────┘└───────────┘»\n",
       "«       ░              ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐ ░          »\n",
       "«qr_0: ─░──■──■────────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[32]) ├┤ Rz(θ[36]) ├─░──■──■────»\n",
       "«       ░  │  │        ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │  │    »\n",
       "«qr_1: ─░──┼──■──■─────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[33]) ├┤ Rz(θ[37]) ├─░──┼──■──■─»\n",
       "«       ░  │     │     ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │     │ »\n",
       "«qr_2: ─░──┼─────■──■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[34]) ├┤ Rz(θ[38]) ├─░──┼─────■─»\n",
       "«       ░  │        │  ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │       »\n",
       "«qr_3: ─░──■────────■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[35]) ├┤ Rz(θ[39]) ├─░──■───────»\n",
       "«       ░              ░ └──────────┘ ░ └───────────┘└───────────┘ ░          »\n",
       "«          ░ ┌──────────┐ ░ ┌───────────┐┌───────────┐ ░              ░ \n",
       "«qr_0: ────░─┤ Rx(x[0]) ├─░─┤ Ry(θ[40]) ├┤ Rz(θ[44]) ├─░──■──■────────░─\n",
       "«          ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │  │        ░ \n",
       "«qr_1: ────░─┤ Rx(x[1]) ├─░─┤ Ry(θ[41]) ├┤ Rz(θ[45]) ├─░──┼──■──■─────░─\n",
       "«          ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │     │     ░ \n",
       "«qr_2: ─■──░─┤ Rx(x[2]) ├─░─┤ Ry(θ[42]) ├┤ Rz(θ[46]) ├─░──┼─────■──■──░─\n",
       "«       │  ░ ├──────────┤ ░ ├───────────┤├───────────┤ ░  │        │  ░ \n",
       "«qr_3: ─■──░─┤ Rx(x[3]) ├─░─┤ Ry(θ[43]) ├┤ Rz(θ[47]) ├─░──■────────■──░─\n",
       "«          ░ └──────────┘ ░ └───────────┘└───────────┘ ░              ░ "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 4\n",
    "qc = parametrized_circuit(num_qubits = num_qubits, \n",
    "                          reuploading = True, \n",
    "                          reps = 6)\n",
    "X = list(qc.parameters)[: num_qubits]\n",
    "params = list(qc.parameters)[num_qubits:]\n",
    "qc.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ecc1a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No gradient function provided, creating a gradient function. If your Estimator requires transpilation, please provide a pass manager.\n"
     ]
    }
   ],
   "source": [
    "#PyTorch Connector\n",
    "estimator = AerEstimator() \n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    input_params=X,\n",
    "    weight_params=params,\n",
    "    estimator=estimator\n",
    ")\n",
    "# Connect to PyTorch\n",
    "initial_weights = (2*np.random.rand(qnn.num_weights) - 1)\n",
    "quantum_nn = TorchConnector(qnn, initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecf3f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoding_layer(torch.nn.Module):\n",
    "    def __init__(self, num_qubits = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define weights for the layer\n",
    "        weights = torch.Tensor(num_qubits)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, -1, 1) \n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step, as explained above.\"\"\"\n",
    "        \n",
    "        if not isinstance(x, Tensor):\n",
    "            x = Tensor(x)\n",
    "        \n",
    "        x = self.weights * x\n",
    "        x = torch.atan(x)\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b49bf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp_val_layer(torch.nn.Module):\n",
    "    def __init__(self, action_space = 2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the weights for the layer\n",
    "        weights = torch.Tensor(action_space)\n",
    "        self.weights = torch.nn.Parameter(weights)\n",
    "        torch.nn.init.uniform_(self.weights, 35, 40) # <-- Initialization strategy (heuristic choice)\n",
    "        \n",
    "        # Masks that map the vector of probabilities to <Z_0*Z_1> and <Z_2*Z_3>\n",
    "        self.mask_ZZ_12 = torch.tensor([1.,-1.,-1.,1.,1.,-1.,-1.,1.,1.,-1.,-1.,1.,1.,-1.,-1.,1.], requires_grad = False)\n",
    "        self.mask_ZZ_34 = torch.tensor([-1.,-1.,-1.,-1.,1.,1.,1.,1.,-1.,-1.,-1.,-1.,1.,1.,1.,1.], requires_grad = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward step, as described above.\"\"\"\n",
    "        \n",
    "        expval_ZZ_12 = self.mask_ZZ_12 * x\n",
    "        expval_ZZ_34 = self.mask_ZZ_34 * x\n",
    "        \n",
    "        # Single sample\n",
    "        if len(x.shape) == 1:\n",
    "            expval_ZZ_12 = torch.sum(expval_ZZ_12)\n",
    "            expval_ZZ_34 = torch.sum(expval_ZZ_34)\n",
    "            out = torch.cat((expval_ZZ_12.unsqueeze(0), expval_ZZ_34.unsqueeze(0)))\n",
    "        \n",
    "        # Batch of samples\n",
    "        else:\n",
    "            expval_ZZ_12 = torch.sum(expval_ZZ_12, dim = 1, keepdim = True)\n",
    "            expval_ZZ_34 = torch.sum(expval_ZZ_34, dim = 1, keepdim = True)\n",
    "            out = torch.cat((expval_ZZ_12, expval_ZZ_34), 1)\n",
    "                \n",
    "        return self.weights * ((out + 1.) / 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0807bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weights', tensor([0.3173, 0.4480, 0.0538, 0.6332])),\n",
       "             ('1.weight',\n",
       "              tensor([ 0.8481, -0.8442, -0.8235,  0.9780,  0.1273, -0.4580, -0.0693, -0.6563,\n",
       "                      -0.2601, -0.8433, -0.6814,  0.8737, -0.5132, -0.2454, -0.1320, -0.7731,\n",
       "                      -0.5694,  0.4126,  0.2250, -0.4415,  0.2136,  0.0754,  0.9722,  0.7280,\n",
       "                       0.9328, -0.7745,  0.6456, -0.3321, -0.3205,  0.1676, -0.8346, -0.1223,\n",
       "                      -0.9597, -0.4486,  0.1689,  0.0298,  0.2180,  0.8072, -0.2345, -0.9390,\n",
       "                      -0.2795,  0.5678,  0.3898,  0.0498,  0.1794, -0.3097,  0.1085, -0.5412])),\n",
       "             ('1._weights',\n",
       "              tensor([ 0.8481, -0.8442, -0.8235,  0.9780,  0.1273, -0.4580, -0.0693, -0.6563,\n",
       "                      -0.2601, -0.8433, -0.6814,  0.8737, -0.5132, -0.2454, -0.1320, -0.7731,\n",
       "                      -0.5694,  0.4126,  0.2250, -0.4415,  0.2136,  0.0754,  0.9722,  0.7280,\n",
       "                       0.9328, -0.7745,  0.6456, -0.3321, -0.3205,  0.1676, -0.8346, -0.1223,\n",
       "                      -0.9597, -0.4486,  0.1689,  0.0298,  0.2180,  0.8072, -0.2345, -0.9390,\n",
       "                      -0.2795,  0.5678,  0.3898,  0.0498,  0.1794, -0.3097,  0.1085, -0.5412])),\n",
       "             ('2.weights', tensor([38.4888, 39.3277]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = encoding_layer()\n",
    "\n",
    "# Classical trainable postprocessing\n",
    "exp_val = exp_val_layer()\n",
    "\n",
    "# Stack the classical and quantum layers together \n",
    "model = torch.nn.Sequential(encoding, \n",
    "                            quantum_nn, \n",
    "                            exp_val)\n",
    "\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4543dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6782913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89412d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "discount_rate = 0.99\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa9caf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    \"\"\"Manages the transition from the *exploration* to *exploitation* phase\"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            Q_values = model(Tensor(state)).numpy()\n",
    "        return np.argmax(Q_values[0])\n",
    "    \n",
    "def sample_experiences(batch_size):\n",
    "    \"\"\"Sample some past experiences from the replay memory\"\"\"\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    # Extract each field separately, handling arrays and scalars appropriately\n",
    "    # Ensure states and next_states are consistently shaped numpy arrays\n",
    "    states_list = []\n",
    "    next_states_list = []\n",
    "    for experience in batch:\n",
    "        # Convert state to numpy array, handling tuples or nested structures\n",
    "        state_val = experience[0]\n",
    "        if isinstance(state_val, tuple):\n",
    "            state_val = state_val[0]  # If it's a tuple, take the first element\n",
    "        state = np.asarray(state_val, dtype=np.float32).flatten()\n",
    "        \n",
    "        # Convert next_state to numpy array, handling tuples or nested structures\n",
    "        next_state_val = experience[3]\n",
    "        if isinstance(next_state_val, tuple):\n",
    "            next_state_val = next_state_val[0]  # If it's a tuple, take the first element\n",
    "        next_state = np.asarray(next_state_val, dtype=np.float32).flatten()\n",
    "        \n",
    "        states_list.append(state)\n",
    "        next_states_list.append(next_state)\n",
    "    states = np.stack(states_list)\n",
    "    actions = np.array([experience[1] for experience in batch])\n",
    "    rewards = np.array([experience[2] for experience in batch], dtype=np.float32)\n",
    "    next_states = np.stack(next_states_list)\n",
    "    dones = np.array([experience[4] for experience in batch], dtype=np.bool_)\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    \"\"\"Perform one action in the environment and register the state of the system\"\"\"\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated  # Gymnasium returns terminated and truncated separately\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "def sequential_training_step(batch_size):\n",
    "    \"\"\"\n",
    "    Actual training routine. Implements the Deep Q-Learning algorithm.\n",
    "    \n",
    "    This implementation evaluates individual losses sequentially instead of using batches. \n",
    "    This is due to an issue in the TorchConnector, which yields vanishing gradients if it \n",
    "    is called with a batch of data (see https://github.com/Qiskit/qiskit-machine-learning/issues/100).\n",
    "    \n",
    "    Use this training for the quantum model. If using the classical model, you can use indifferently \n",
    "    this implementation or the batched one below. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample past experiences \n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Evaluates Target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_Q_values = model(Tensor(next_states)).numpy()\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards + (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    \n",
    "    # Accumulate Loss sequentially (if batching data, gradients of the parameters are vanishing)\n",
    "    loss = 0.\n",
    "    for j, state in enumerate(states):\n",
    "        single_Q_value = model(Tensor(state))\n",
    "        Q_value = single_Q_value[actions[j]]\n",
    "        loss += (target_Q_values[j] - Q_value)**2\n",
    "    \n",
    "    # Evaluate the gradients and update the parameters \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    \"\"\"\n",
    "    This is exactly the same function as sequential_training_step, except that it \n",
    "    evaluates loss with batch of data, instead of using a for loop. \n",
    "    \n",
    "    Can use this if training the classical model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample past experiences\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Evaluate Target Q-values\n",
    "    with torch.no_grad():\n",
    "        next_Q_values = model(Tensor(next_states)).numpy()\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = torch.nn.functional.one_hot(Tensor(actions).long(), n_outputs)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    all_Q_values = model(Tensor(states))\n",
    "    Q_values = torch.sum(all_Q_values * mask, dim=1, keepdims=True)\n",
    "    loss = loss_fn(Tensor(target_Q_values), Q_values)\n",
    "    \n",
    "    # Evaluate the gradients and update the parameters \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3f7335f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 25, Steps : 14, eps: 0.983"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Start training only after some exploration experiences  \u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode > \u001b[32m20\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43msequential_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36msequential_training_step\u001b[39m\u001b[34m(batch_size)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Evaluate the gradients and update the parameters \u001b[39;00m\n\u001b[32m     78\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/torch/autograd/function.py:315\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m     )\n\u001b[32m    314\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/qiskit_machine_learning/connectors/torch_connector.py:229\u001b[39m, in \u001b[36m_TorchNNFunction.backward\u001b[39m\u001b[34m(ctx, grad_output)\u001b[39m\n\u001b[32m    226\u001b[39m     grad_output = grad_output.view(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# evaluate QNN gradient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m input_grad, weights_grad = \u001b[43mneural_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ctx.sparse:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/qiskit_machine_learning/neural_networks/neural_network.py:257\u001b[39m, in \u001b[36mNeuralNetwork.backward\u001b[39m\u001b[34m(self, input_data, weights)\u001b[39m\n\u001b[32m    255\u001b[39m input_, shape = \u001b[38;5;28mself\u001b[39m._validate_input(input_data)\n\u001b[32m    256\u001b[39m weights_ = \u001b[38;5;28mself\u001b[39m._validate_weights(weights)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m input_grad, weight_grad = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m input_grad_reshaped, weight_grad_reshaped = \u001b[38;5;28mself\u001b[39m._validate_backward_output(\n\u001b[32m    260\u001b[39m     input_grad, weight_grad, shape\n\u001b[32m    261\u001b[39m )\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m input_grad_reshaped, weight_grad_reshaped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/qiskit_machine_learning/neural_networks/estimator_qnn.py:364\u001b[39m, in \u001b[36mEstimatorQNN._backward\u001b[39m\u001b[34m(self, input_data, weights)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m         results = \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m QiskitMachineLearningError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEstimator job failed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Semester 5/QC/QiRL/venv/lib/python3.13/site-packages/qiskit/primitives/primitive_job.py:51\u001b[39m, in \u001b[36mPrimitiveJob.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ResultT:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_submitted()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "# We let the agent train for 2000 episodes\n",
    "for episode in range(2000):\n",
    "    \n",
    "    # Run enviroment simulation\n",
    "    obs, info = env.reset()  # Gymnasium returns (observation, info)\n",
    "    \n",
    "    # 200 is the target score for considering the environment solved\n",
    "    for step in range(200):\n",
    "        \n",
    "        # Manages the transition from exploration to exploitation\n",
    "        epsilon = max(1 - episode / 1500, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step)\n",
    "    \n",
    "    # Saving best agent\n",
    "    if step >= best_score:\n",
    "        # torch.save(model.state_dict(), './new_model_best_weights.pth') # Save best weights\n",
    "        best_score = step\n",
    "        \n",
    "    print(\"\\rEpisode: {}, Steps : {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\")\n",
    "    \n",
    "    # Start training only after some exploration experiences  \n",
    "    if episode > 20:\n",
    "        sequential_training_step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49902057",
   "metadata": {},
   "source": [
    "### Post Training Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c07683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "# This file contains the sum of rewards from a previous successfull training run\n",
    "rewards_history = np.loadtxt(\"training_rewards2.txt\") + 1\n",
    "\n",
    "cmap = plt.get_cmap('tab20c')\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.axhline([200], ls = 'dashed', c=cmap(9))\n",
    "plt.text(-50,190, s='Max reward', c=cmap(8))\n",
    "\n",
    "plt.text(-50,100, s='Exploration\\nphase', c=cmap(12))\n",
    "plt.text(1100,100, s='Exploitation\\nphase', c=cmap(12))\n",
    "\n",
    "plt.plot(rewards_history, c = cmap(4))\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Final reward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(42)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "frames = []\n",
    "for step in range(200):\n",
    "    action = epsilon_greedy_policy(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        print(\"End at step:\", step)\n",
    "        break\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
